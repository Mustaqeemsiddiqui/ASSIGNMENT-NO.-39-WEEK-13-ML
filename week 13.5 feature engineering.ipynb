{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5b6710e-db22-4d37-ad0d-1f1d6b596b56",
   "metadata": {},
   "source": [
    "**Q1.What is data encoding? How is it useful in data science?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2658646f-1984-4c04-8ac8-8a74353f7655",
   "metadata": {},
   "source": [
    "**ANSWER:----**\n",
    "\n",
    "Data encoding refers to the process of converting data into a specific format that can be easily stored, transmitted, and processed. In the context of data science, encoding is crucial for transforming raw data into a format that can be effectively used by machine learning algorithms and analytical models. There are several types of data encoding, each serving different purposes and suitable for different types of data. Here are a few common types:\n",
    "\n",
    "1. **Label Encoding**:\n",
    "   - Converts categorical data into numerical labels.\n",
    "   - Useful when the categorical variable is ordinal (i.e., the categories have a natural order).\n",
    "\n",
    "2. **One-Hot Encoding**:\n",
    "   - Converts categorical data into a binary matrix.\n",
    "   - Each category is represented as a binary vector, which is useful for non-ordinal categorical data where no order is implied.\n",
    "\n",
    "3. **Binary Encoding**:\n",
    "   - Encodes categories as binary numbers.\n",
    "   - Reduces the dimensionality compared to one-hot encoding, useful for high cardinality categorical variables.\n",
    "\n",
    "4. **Hash Encoding**:\n",
    "   - Uses a hash function to convert categories to a fixed number of columns.\n",
    "   - Useful for handling high cardinality categorical variables without creating a very large number of columns.\n",
    "\n",
    "5. **Frequency/Count Encoding**:\n",
    "   - Encodes categories based on their frequency or count in the dataset.\n",
    "   - Helps in situations where the frequency of occurrence of categories is an important feature.\n",
    "\n",
    "6. **Target/Mean Encoding**:\n",
    "   - Replaces categories with the mean of the target variable for each category.\n",
    "   - Commonly used in supervised learning to introduce some information about the target variable.\n",
    "\n",
    "### Utility in Data Science\n",
    "\n",
    "1. **Compatibility with Machine Learning Algorithms**:\n",
    "   - Many machine learning algorithms require numerical input, and encoding categorical data into numerical values makes it possible to use such algorithms.\n",
    "\n",
    "2. **Improving Model Performance**:\n",
    "   - Proper encoding can help capture the underlying patterns in the data more effectively, leading to improved model accuracy and performance.\n",
    "\n",
    "3. **Handling Different Data Types**:\n",
    "   - Encoding allows for the seamless integration of categorical, ordinal, and nominal data into the modeling process, ensuring that no valuable information is lost.\n",
    "\n",
    "4. **Dimensionality Reduction**:\n",
    "   - Encoding techniques like binary encoding and hash encoding help in reducing the dimensionality of the dataset, making it more manageable and efficient for processing.\n",
    "\n",
    "5. **Mitigating Overfitting**:\n",
    "   - Techniques like target encoding help in mitigating overfitting by smoothing the target variable's impact across categories.\n",
    "\n",
    "6. **Data Compression**:\n",
    "   - Encoding can also be used to compress data, reducing storage requirements and improving the efficiency of data transmission.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415dab77-0dba-467a-8653-03c1cfeba3b2",
   "metadata": {},
   "source": [
    "**Q2. What is nominal encoding? Provide an example of how you would use it in a real-world scenario.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456f0363-5d8e-4e6f-a5a9-adaf15a74005",
   "metadata": {},
   "source": [
    "**ANSWER:-----**\n",
    "\n",
    "Nominal encoding refers to the process of converting nominal categorical variables into a numerical format that can be utilized by machine learning algorithms. Nominal categorical variables are those that have two or more categories, but there is no intrinsic ordering to these categories (e.g., color, brand, type).\n",
    "\n",
    "The most common methods for nominal encoding are:\n",
    "\n",
    "1. **One-Hot Encoding**: Converts each category into a binary vector where only one bit is '1' (indicating the presence of the category) and all others are '0'.\n",
    "2. **Label Encoding**: Assigns each category a unique integer, but this method can introduce ordinal relationships which may not be desirable for nominal data.\n",
    "\n",
    "### Example of Nominal Encoding in a Real-World Scenario\n",
    "\n",
    "**Scenario**: Suppose you are working on a machine learning project to predict customer churn for a telecommunications company. One of the features in your dataset is the \"Preferred Contact Method\" of customers, which includes categories like \"Email,\" \"Phone,\" \"SMS,\" and \"None.\"\n",
    "\n",
    "#### Using One-Hot Encoding\n",
    "\n",
    "Given the categories: [\"Email,\" \"Phone,\" \"SMS,\" \"None\"], one-hot encoding would transform this feature as follows:\n",
    "\n",
    "| Preferred Contact Method | Email | Phone | SMS | None |\n",
    "|--------------------------|-------|-------|-----|------|\n",
    "| Email                    | 1     | 0     | 0   | 0    |\n",
    "| Phone                    | 0     | 1     | 0   | 0    |\n",
    "| SMS                      | 0     | 0     | 1   | 0    |\n",
    "| None                     | 0     | 0     | 0   | 1    |\n",
    "\n",
    "#### Using Label Encoding (less preferred for nominal data)\n",
    "\n",
    "Given the same categories, label encoding would assign an integer to each:\n",
    "\n",
    "| Preferred Contact Method | Encoded Value |\n",
    "|--------------------------|---------------|\n",
    "| Email                    | 1             |\n",
    "| Phone                    | 2             |\n",
    "| SMS                      | 3             |\n",
    "| None                     | 4             |\n",
    "\n",
    "**Application**:\n",
    "1. **Data Preparation**:\n",
    "   - Import the necessary libraries.\n",
    "   - Load the dataset containing the \"Preferred Contact Method\" feature.\n",
    "   - Apply one-hot encoding to this feature.\n",
    "\n",
    "2. **Implementation**:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3499af9c-6c69-4a0b-ad55-5ab4cb8496e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CustomerID  Preferred Contact Method_Email  Preferred Contact Method_None  \\\n",
      "0           1                             1.0                            0.0   \n",
      "1           2                             0.0                            0.0   \n",
      "2           3                             0.0                            0.0   \n",
      "3           4                             0.0                            1.0   \n",
      "\n",
      "   Preferred Contact Method_Phone  Preferred Contact Method_SMS  \n",
      "0                             0.0                           0.0  \n",
      "1                             1.0                           0.0  \n",
      "2                             0.0                           1.0  \n",
      "3                             0.0                           0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:808: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Sample dataset\n",
    "data = {'CustomerID': [1, 2, 3, 4],\n",
    "        'Preferred Contact Method': ['Email', 'Phone', 'SMS', 'None']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the encoder\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the data\n",
    "encoded_data = one_hot_encoder.fit_transform(df[['Preferred Contact Method']])\n",
    "\n",
    "# Create a DataFrame with the encoded data\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=one_hot_encoder.get_feature_names_out(['Preferred Contact Method']))\n",
    "\n",
    "# Concatenate the original dataframe with the encoded dataframe\n",
    "df_encoded = pd.concat([df, encoded_df], axis=1).drop('Preferred Contact Method', axis=1)\n",
    "\n",
    "print(df_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a795fcc-59a5-4e92-9260-d2c64c49a82a",
   "metadata": {},
   "source": [
    "**Q3. In what situations is nominal encoding preferred over one-hot encoding? Provide a practical example.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df815a7f-3291-44d4-a9f2-5fec96c13df2",
   "metadata": {},
   "source": [
    "**ANSWER:------**\n",
    "\n",
    "\n",
    "Nominal encoding is often preferred over one-hot encoding in situations where the categorical variable has a high cardinality, meaning it contains a large number of unique categories. One-hot encoding can lead to a very large and sparse matrix in such cases, which can increase the computational burden and memory usage, potentially leading to inefficiencies in model training and inference.\n",
    "\n",
    "### Situations Where Nominal Encoding is Preferred\n",
    "\n",
    "1. **High Cardinality Features**:\n",
    "   - When the categorical feature has a large number of unique values, such as ZIP codes, product IDs, or user IDs, nominal encoding can help reduce the dimensionality of the dataset.\n",
    "\n",
    "2. **Tree-Based Algorithms**:\n",
    "   - Some tree-based algorithms (like decision trees, random forests, and gradient boosting machines) can handle label encoded data effectively and do not require one-hot encoding.\n",
    "\n",
    "3. **Memory and Computation Constraints**:\n",
    "   - When computational resources and memory are limited, reducing the number of features through nominal encoding can be beneficial.\n",
    "\n",
    "### Practical Example\n",
    "\n",
    "**Scenario**: Imagine you are working on a recommendation system for an e-commerce platform. The dataset includes a feature called \"Product ID,\" which has thousands of unique product IDs.\n",
    "\n",
    "#### Using Label Encoding (a form of nominal encoding)\n",
    "\n",
    "Given the high cardinality of the \"Product ID\" feature, label encoding will assign a unique integer to each product ID. This approach reduces the complexity and size of the dataset compared to one-hot encoding.\n",
    "\n",
    "**Steps**:\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - Import the necessary libraries.\n",
    "   - Load the dataset containing the \"Product ID\" feature.\n",
    "   - Apply label encoding to this feature.\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26469cb8-5be3-4b6f-87dc-d1f4ca3aef0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UserID ProductID  ProductID_Encoded\n",
      "0       1      P123                  0\n",
      "1       2      P456                  1\n",
      "2       3      P789                  2\n",
      "3       4      P123                  0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sample dataset\n",
    "data = {'UserID': [1, 2, 3, 4],\n",
    "        'ProductID': ['P123', 'P456', 'P789', 'P123']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the data\n",
    "df['ProductID_Encoded'] = label_encoder.fit_transform(df['ProductID'])\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c97a4d4-8019-4bbb-b4c9-9538ec7788c1",
   "metadata": {},
   "source": [
    "\n",
    "2. **Implementation**:\n",
    "   \n",
    "In this example, the \"Product ID\" feature is transformed into numerical values using label encoding. This approach is efficient because it avoids the high dimensionality problem associated with one-hot encoding.\n",
    "\n",
    "### Comparison with One-Hot Encoding\n",
    "\n",
    "If we had used one-hot encoding for the \"Product ID\" feature, the dataset would expand significantly with additional columns for each unique product ID, which is impractical with thousands of products.\n",
    "\n",
    "**Pros of Nominal Encoding**:\n",
    "- Reduces dimensionality.\n",
    "- Efficient in terms of memory and computation.\n",
    "- Suitable for tree-based algorithms.\n",
    "\n",
    "**Cons of Nominal Encoding**:\n",
    "- Introduces ordinal relationships that might not be meaningful for certain algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7adc2f-d1ed-4587-8869-1fb4ca1ce525",
   "metadata": {},
   "source": [
    "**Q4. Suppose you have a dataset containing categorical data with 5 unique values. Which encoding\n",
    "technique would you use to transform this data into a format suitable for machine learning algorithms?\n",
    "Explain why you made this choice.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42254d01-6032-44e0-8bfa-7dfc60eabeab",
   "metadata": {},
   "source": [
    "**ANSWER:-----**\n",
    "\n",
    "Given a dataset containing categorical data with 5 unique values, the choice of encoding technique will depend on several factors, including the type of machine learning algorithm being used, the nature of the categorical data, and the potential impact of introducing ordinal relationships. Here are the most suitable encoding techniques for this scenario and the rationale behind each choice:\n",
    "\n",
    "### One-Hot Encoding\n",
    "\n",
    "**Description**: One-hot encoding converts each category into a binary vector, where only one bit is '1' (indicating the presence of the category) and all others are '0'.\n",
    "\n",
    "**Implementation**:\n",
    "- Use the `OneHotEncoder` from scikit-learn.\n",
    "\n",
    "**Advantages**:\n",
    "- **Avoids Ordinal Relationships**: One-hot encoding does not assume any ordinal relationship between categories, making it suitable for nominal data.\n",
    "- **Compatibility**: It is widely compatible with most machine learning algorithms, including linear models, neural networks, and distance-based algorithms like k-NN.\n",
    "\n",
    "**Disadvantages**:\n",
    "- **Increased Dimensionality**: Adds more columns to the dataset, which can be a concern with high cardinality features but is manageable with only 5 unique values.\n",
    "\n",
    "\n",
    "### Label Encoding\n",
    "\n",
    "**Description**: Label encoding assigns a unique integer to each category.\n",
    "\n",
    "**Implementation**:\n",
    "- Use the `LabelEncoder` from scikit-learn.\n",
    "\n",
    "**Advantages**:\n",
    "- **Simplicity**: Easy to implement and does not increase the dimensionality of the dataset.\n",
    "- **Efficiency**: Suitable for tree-based algorithms like decision trees and random forests, which can handle label encoded data effectively.\n",
    "\n",
    "**Disadvantages**:\n",
    "- **Ordinal Relationship**: Introduces an ordinal relationship between categories, which might not be meaningful for some algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3bc53b6-9741-410f-bab8-7e3d47b15bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Category_A  Category_B  Category_C  Category_D  Category_E\n",
      "0         1.0         0.0         0.0         0.0         0.0\n",
      "1         0.0         1.0         0.0         0.0         0.0\n",
      "2         0.0         0.0         1.0         0.0         0.0\n",
      "3         0.0         0.0         0.0         1.0         0.0\n",
      "4         0.0         0.0         0.0         0.0         1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:808: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Sample dataset\n",
    "data = {'Category': ['A', 'B', 'C', 'D', 'E']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the encoder\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the data\n",
    "encoded_data = one_hot_encoder.fit_transform(df[['Category']])\n",
    "\n",
    "# Create a DataFrame with the encoded data\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=one_hot_encoder.get_feature_names_out(['Category']))\n",
    "\n",
    "# Concatenate the original dataframe with the encoded dataframe\n",
    "df_encoded = pd.concat([df, encoded_df], axis=1).drop('Category', axis=1)\n",
    "\n",
    "print(df_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "129075dc-62bb-473a-8e8c-df11b9d476d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Category  Category_Encoded\n",
      "0        A                 0\n",
      "1        B                 1\n",
      "2        C                 2\n",
      "3        D                 3\n",
      "4        E                 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the data\n",
    "df['Category_Encoded'] = label_encoder.fit_transform(df['Category'])\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d5205f-626a-4049-bbd9-f47e9d1b22f3",
   "metadata": {},
   "source": [
    "**Q5. In a machine learning project, you have a dataset with 1000 rows and 5 columns. Two of the columns\n",
    "are categorical, and the remaining three columns are numerical. If you were to use nominal encoding to\n",
    "transform the categorical data, how many new columns would be created? Show your calculations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1b06a4-1cac-4604-9ffb-7e34a2d90acd",
   "metadata": {},
   "source": [
    "**ANSWER:------**\n",
    "\n",
    "To determine the number of new columns created by nominal encoding (specifically, one-hot encoding) for the two categorical columns, we need to know the number of unique categories in each of these columns. Since this information is not provided, I'll demonstrate the calculation assuming hypothetical values.\n",
    "\n",
    "Let's assume:\n",
    "- The first categorical column (Cat1) has \\(k_1\\) unique categories.\n",
    "- The second categorical column (Cat2) has \\(k_2\\) unique categories.\n",
    "\n",
    "### One-Hot Encoding Calculation\n",
    "\n",
    "1. **For Cat1**:\n",
    "   - One-hot encoding will create \\(k_1\\) binary columns.\n",
    "\n",
    "2. **For Cat2**:\n",
    "   - One-hot encoding will create \\(k_2\\) binary columns.\n",
    "\n",
    "The total number of new columns created by one-hot encoding the two categorical columns would be:\n",
    "\\[ \\text{Total new columns} = k_1 + k_2 \\]\n",
    "\n",
    "### Example Calculation\n",
    "\n",
    "Suppose:\n",
    "- Cat1 has 4 unique categories.\n",
    "- Cat2 has 3 unique categories.\n",
    "\n",
    "The calculation would be:\n",
    "\\[ \\text{Total new columns} = 4 + 3 = 7 \\]\n",
    "\n",
    "### Updated Dataset Structure\n",
    "\n",
    "The original dataset has 5 columns:\n",
    "- Cat1\n",
    "- Cat2\n",
    "- Num1 (numerical)\n",
    "- Num2 (numerical)\n",
    "- Num3 (numerical)\n",
    "\n",
    "After one-hot encoding the categorical columns:\n",
    "- The 2 original categorical columns are replaced by the 7 new binary columns.\n",
    "- The 3 numerical columns remain unchanged.\n",
    "\n",
    "Therefore, the new dataset will have:\n",
    "\\[ \\text{Total columns in the new dataset} = 3 \\text{ (original numerical columns)} + 7 \\text{ (new one-hot encoded columns)} = 10 \\text{ columns} \\]\n",
    "\n",
    "### General Formula\n",
    "\n",
    "If:\n",
    "- \\( k_1 \\) is the number of unique categories in the first categorical column.\n",
    "- \\( k_2 \\) is the number of unique categories in the second categorical column.\n",
    "\n",
    "The total number of columns in the transformed dataset would be:\n",
    "\\[ \\text{Total columns in the new dataset} = 3 + k_1 + k_2 \\]\n",
    "\n",
    "Without the exact number of unique categories in Cat1 and Cat2, this is the general method to calculate the new number of columns created by nominal (one-hot) encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9294a2f5-87d9-47fe-a260-13ff6bfe15fd",
   "metadata": {},
   "source": [
    "**Q6. You are working with a dataset containing information about different types of animals, including their\n",
    "species, habitat, and diet. Which encoding technique would you use to transform the categorical data into\n",
    "a format suitable for machine learning algorithms? Justify your answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a29393-0d5d-4c8a-9352-dd246e38497e",
   "metadata": {},
   "source": [
    "**ANSWER:-----**\n",
    "\n",
    "When working with a dataset containing information about different types of animals, including their species, habitat, and diet, the choice of encoding technique should consider the nature of the categorical data, the number of unique categories, and the requirements of the machine learning algorithms. Given the typical characteristics of these features, a mixed approach using both one-hot encoding and possibly label encoding or binary encoding might be appropriate.\n",
    "\n",
    "### Analysis of Categorical Features\n",
    "1. **Species**:\n",
    "   - Likely nominal with a potentially large number of unique categories (e.g., lion, tiger, elephant).\n",
    "   - A higher cardinality feature.\n",
    "\n",
    "2. **Habitat**:\n",
    "   - Likely nominal with fewer unique categories (e.g., forest, savannah, ocean).\n",
    "   - Lower cardinality feature.\n",
    "\n",
    "3. **Diet**:\n",
    "   - Likely nominal with fewer unique categories (e.g., herbivore, carnivore, omnivore).\n",
    "   - Lower cardinality feature.\n",
    "\n",
    "### Recommended Encoding Techniques\n",
    "1. **One-Hot Encoding for Habitat and Diet**:\n",
    "   - **Reason**: These features likely have a small number of unique categories, making one-hot encoding a practical choice. One-hot encoding effectively handles nominal data without imposing any ordinal relationship and keeps the feature space manageable.\n",
    "   \n",
    "2. **Binary Encoding or Label Encoding for Species**:\n",
    "   - **Reason**: If the species feature has a large number of unique categories, one-hot encoding can lead to a very high-dimensional and sparse dataset. Binary encoding can reduce the dimensionality compared to one-hot encoding while retaining sufficient information. Label encoding is another option if the downstream algorithm can handle ordinal relationships, but typically, binary encoding is preferred for high-cardinality nominal data.\n",
    "\n",
    "### Practical Example\n",
    "\n",
    "Assume:\n",
    "- Species has 20 unique categories.\n",
    "- Habitat has 5 unique categories.\n",
    "- Diet has 3 unique categories.\n",
    "\n",
    "Here’s how you would apply the encoding techniques:\n",
    "\n",
    "### Implementation Steps\n",
    "1. **Data Preparation**:\n",
    "   - Import the necessary libraries.\n",
    "   - Load the dataset.\n",
    "   - Apply one-hot encoding to Habitat and Diet.\n",
    "   - Apply binary encoding to Species.\n",
    "\n",
    "2. **Implementation**:\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc163ebc-a41f-4e05-b688-ab9c245f2f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting category_encoders\n",
      "  Downloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: patsy>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from category_encoders) (0.5.3)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from category_encoders) (0.13.5)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /opt/conda/lib/python3.10/site-packages (from category_encoders) (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from category_encoders) (1.9.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from category_encoders) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from category_encoders) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.5->category_encoders) (2022.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->category_encoders) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->category_encoders) (3.1.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from statsmodels>=0.9.0->category_encoders) (22.0)\n",
      "Installing collected packages: category_encoders\n",
      "Successfully installed category_encoders-2.6.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install category_encoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a9d5de9-9e37-4d3f-b628-286f7419c4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Species_0  Species_1  Species_2  Habitat_Forest  Habitat_Savannah  \\\n",
      "0          0          0          1             0.0               1.0   \n",
      "1          0          1          0             1.0               0.0   \n",
      "2          0          1          1             0.0               1.0   \n",
      "3          0          0          1             0.0               1.0   \n",
      "4          1          0          0             0.0               1.0   \n",
      "\n",
      "   Diet_Carnivore  Diet_Herbivore  \n",
      "0             1.0             0.0  \n",
      "1             1.0             0.0  \n",
      "2             0.0             1.0  \n",
      "3             1.0             0.0  \n",
      "4             0.0             1.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import category_encoders as ce\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'Species': ['Lion', 'Tiger', 'Elephant', 'Lion', 'Zebra'],\n",
    "    'Habitat': ['Savannah', 'Forest', 'Savannah', 'Savannah', 'Savannah'],\n",
    "    'Diet': ['Carnivore', 'Carnivore', 'Herbivore', 'Carnivore', 'Herbivore']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize one-hot encoder for Habitat and Diet\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "habitat_diet_encoded = one_hot_encoder.fit_transform(df[['Habitat', 'Diet']])\n",
    "habitat_diet_encoded_df = pd.DataFrame(habitat_diet_encoded, columns=one_hot_encoder.get_feature_names_out(['Habitat', 'Diet']))\n",
    "\n",
    "# Initialize binary encoder for Species\n",
    "binary_encoder = ce.BinaryEncoder(cols=['Species'])\n",
    "species_encoded = binary_encoder.fit_transform(df[['Species']])\n",
    "\n",
    "# Concatenate the original dataframe with the encoded dataframes\n",
    "df_encoded = pd.concat([species_encoded, habitat_diet_encoded_df], axis=1)\n",
    "\n",
    "print(df_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24afffda-257e-43fb-870a-9b1f83340cd5",
   "metadata": {},
   "source": [
    "### Justification\n",
    "1. **One-Hot Encoding for Habitat and Diet**:\n",
    "   - **Suitability**: These features have a small number of categories, making one-hot encoding practical without resulting in a large number of columns.\n",
    "   - **No Ordinal Relationships**: One-hot encoding does not impose any ordinal relationships, which is appropriate for nominal data.\n",
    "\n",
    "2. **Binary Encoding for Species**:\n",
    "   - **Handling High Cardinality**: Species might have many unique categories, and binary encoding helps manage this by reducing the number of new columns compared to one-hot encoding.\n",
    "   - **Efficiency**: This keeps the dataset more compact and efficient for modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da83cc12-1fc0-49bc-8496-27e1535179f0",
   "metadata": {},
   "source": [
    "**Q7.You are working on a project that involves predicting customer churn for a telecommunications\n",
    "company. You have a dataset with 5 features, including the customer's gender, age, contract type,\n",
    "monthly charges, and tenure. Which encoding technique(s) would you use to transform the categorical\n",
    "data into numerical data? Provide a step-by-step explanation of how you would implement the encoding.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a8248b-1337-4d16-b8d7-81edee326027",
   "metadata": {},
   "source": [
    "**ANSWER:-----**\n",
    "\n",
    "To transform the categorical data in your dataset into numerical data suitable for machine learning models, we need to consider the nature of each categorical feature and choose appropriate encoding techniques. Here’s a step-by-step explanation of how you can implement encoding for each feature in your dataset:\n",
    "\n",
    "### Dataset Features\n",
    "1. **Gender**: Categorical (likely binary: Male/Female or other)\n",
    "2. **Contract type**: Categorical with multiple categories (e.g., month-to-month, one year, two year)\n",
    "3. **Other numerical features**:\n",
    "   - **Age**: Continuous numerical\n",
    "   - **Monthly charges**: Continuous numerical\n",
    "   - **Tenure**: Continuous numerical\n",
    "\n",
    "### Recommended Encoding Techniques\n",
    "\n",
    "#### 1. Gender (Binary Categorical)\n",
    "- Since gender is typically binary (e.g., Male/Female), we can use **binary encoding** or **label encoding**.\n",
    "- **Binary Encoding**: Transforms the gender feature into a binary format (0/1).\n",
    "\n",
    "#### 2. Contract Type (Multi-category Categorical)\n",
    "- Contract type has multiple categories (e.g., month-to-month, one year, two year). \n",
    "- **One-Hot Encoding**: Use one-hot encoding because contract type does not have a natural ordinal relationship.\n",
    "  - One-hot encoding will create separate binary columns for each category, indicating whether a customer has a specific contract type.\n",
    "\n",
    "#### 3. Numerical Features (Age, Monthly Charges, Tenure)\n",
    "- These features are already numerical and typically do not require further encoding.\n",
    "- Ensure these features are properly scaled if necessary (e.g., using standardization or normalization) before feeding them into machine learning models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1660e626-199e-494e-9aab-d0e74743b6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Gender  Age  MonthlyCharges  Tenure  Contract_One year  Contract_Two year\n",
      "0       1   25            50.0      12                0.0                0.0\n",
      "1       0   30            70.0      24                1.0                0.0\n",
      "2       1   40            60.0       6                0.0                0.0\n",
      "3       0   35            80.0      36                0.0                1.0\n",
      "4       1   28            55.0      18                0.0                1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:808: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Sample dataset (replace with your actual dataset)\n",
    "data = {\n",
    "    'Gender': ['Male', 'Female', 'Male', 'Female', 'Male'],\n",
    "    'Contract': ['Month-to-month', 'One year', 'Month-to-month', 'Two year', 'Two year'],\n",
    "    'Age': [25, 30, 40, 35, 28],\n",
    "    'MonthlyCharges': [50.0, 70.0, 60.0, 80.0, 55.0],\n",
    "    'Tenure': [12, 24, 6, 36, 18]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Binary encoding for Gender\n",
    "label_encoder = LabelEncoder()\n",
    "df['Gender'] = label_encoder.fit_transform(df['Gender'])  # Male -> 1, Female -> 0\n",
    "\n",
    "# Step 2: One-hot encoding for Contract type\n",
    "contract_encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "contract_encoded = contract_encoder.fit_transform(df[['Contract']])\n",
    "contract_encoded_df = pd.DataFrame(contract_encoded, columns=contract_encoder.get_feature_names_out(['Contract']))\n",
    "df = pd.concat([df, contract_encoded_df], axis=1)\n",
    "df.drop('Contract', axis=1, inplace=True)  # Drop the original Contract column after encoding\n",
    "\n",
    "# Step 3: Verify the transformed dataset\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78016b2f-327e-4d20-9c78-83d6de4f1e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Gender  Age  MonthlyCharges  Tenure  Contract_One year  Contract_Two year\n",
      "0       1   25            50.0      12                0.0                0.0\n",
      "1       0   30            70.0      24                1.0                0.0\n",
      "2       1   40            60.0       6                0.0                0.0\n",
      "3       0   35            80.0      36                0.0                1.0\n",
      "4       1   28            55.0      18                0.0                1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Sample dataset (replace with your actual dataset)\n",
    "data = {\n",
    "    'Gender': ['Male', 'Female', 'Male', 'Female', 'Male'],\n",
    "    'Contract': ['Month-to-month', 'One year', 'Month-to-month', 'Two year', 'Two year'],\n",
    "    'Age': [25, 30, 40, 35, 28],\n",
    "    'MonthlyCharges': [50.0, 70.0, 60.0, 80.0, 55.0],\n",
    "    'Tenure': [12, 24, 6, 36, 18]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Binary encoding for Gender\n",
    "label_encoder = LabelEncoder()\n",
    "df['Gender'] = label_encoder.fit_transform(df['Gender'])  # Male -> 1, Female -> 0\n",
    "\n",
    "# Step 2: One-hot encoding for Contract type\n",
    "contract_encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "contract_encoded = contract_encoder.fit_transform(df[['Contract']])\n",
    "contract_encoded_df = pd.DataFrame(contract_encoded, columns=contract_encoder.get_feature_names_out(['Contract']))\n",
    "df = pd.concat([df, contract_encoded_df], axis=1)\n",
    "df.drop('Contract', axis=1, inplace=True)  # Drop the original Contract column after encoding\n",
    "\n",
    "# Step 3: Verify the transformed dataset\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee62351-bde6-4011-9eba-6429e9380a8f",
   "metadata": {},
   "source": [
    "\n",
    "### Output Explanation\n",
    "- **Gender**: Transformed using `LabelEncoder`, where Male is represented as 1 and Female as 0.\n",
    "- **Contract type**: Transformed using `OneHotEncoder`, creating new columns (`x0_One year`, `x0_Two year`) indicating the presence of each contract type.\n",
    "- **Age, Monthly Charges, Tenure**: These features remain unchanged as they are already numerical.\n",
    "\n",
    "### Considerations\n",
    "- Ensure that the encoding process is applied consistently across training and test datasets.\n",
    "- For machine learning algorithms that are sensitive to scale (like SVMs or k-NN), consider scaling numerical features using techniques like normalization or standardization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8142e0a-301c-44a5-adea-b737486ec3da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
